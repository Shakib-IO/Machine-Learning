{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opposite-inquiry",
   "metadata": {},
   "source": [
    "# Neural Network Models for Combined Classification and Regression\n",
    "\n",
    "Some prediction problems require predicting both numeric values and a class label for the same input.\n",
    "\n",
    "A simple approach is to develop both regression and classification predictive models on the same data and use the models sequentially.\n",
    "\n",
    "An alternative and often more effective approach is to develop a single neural network model that can predict both a numeric and class label value from the same input. This is called a multi-output model and can be relatively easy to develop and evaluate using modern deep learning libraries such as Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-impossible",
   "metadata": {},
   "source": [
    "**Single Model for Regression and Classification**\n",
    "\n",
    "It is common to develop a deep learning neural network model for a regression or classification problem, but on some predictive modeling tasks, we may want to develop a single model that can make both regression and classification predictions.\n",
    "\n",
    "Regression refers to predictive modeling problems that involve predicting a numeric value given an input.\n",
    "Classification refers to predictive modeling problems that involve predicting a class label or probability of class labels for a given input.\n",
    "There may be some problems where we want to predict both a numerical value and a classification value.\n",
    "\n",
    "One approach to solving this problem is to develop a separate model for each prediction that is required.\n",
    "The problem with this approach is that the predictions made by the separate models may diverge.\n",
    "An alternate approach that can be used when using neural network models is to develop a single model capable of making separate predictions for a numeric and class output for the same input.\n",
    "\n",
    "**This is called a multi-output neural network model.**\n",
    "\n",
    "The benefit of this type of model is that we have a single model to develop and maintain instead of two models and that training and updating the model on both output types at the same time may offer more consistency in the predictions between the two output types.\n",
    "\n",
    "We will develop a multi-output neural network model capable of making regression and classification predictions at the same time.\n",
    "\n",
    "First, let’s select a dataset where this requirement makes sense and start by developing separate models for both regression and classification predictions.\n",
    "\n",
    "**Separate Regression and Classification Models**\n",
    "\n",
    "In this section, we will start by selecting a real dataset where we may want regression and classification predictions at the same time, then develop separate models for each type of prediction.\n",
    "\n",
    "**Abalone Dataset**\n",
    "We will use the “abalone” dataset.\n",
    "\n",
    "Determining the age of an abalone is a time-consuming task and it is desirable to determine the age from physical details alone.\n",
    "\n",
    "This is a dataset that describes the physical details of abalone and requires predicting the number of rings of the abalone, which is a proxy for the age of the creature.\n",
    "\n",
    "You can learn more about the dataset from here:\n",
    "\n",
    "- Dataset (abalone.csv)\n",
    "- Dataset Details (abalone.names)\n",
    "The “age” can be predicted as both a numerical value (in years) or a class label (ordinal year as a class).\n",
    "\n",
    "No need to download the dataset as we will download it automatically as part of the worked examples.\n",
    "The dataset provides an example of a dataset where we may want both a numerical and classification of an input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ordinary-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 9)\n",
      "   0      1      2      3       4       5       6      7   8\n",
      "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
      "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
      "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
      "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
      "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "# summarize shape\n",
    "print(dataframe.shape)\n",
    "# summarize first few lines\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-peninsula",
   "metadata": {},
   "source": [
    "We can see that there are 4,177 examples (rows) that we can use to train and evaluate a model and 9 features (columns) including the target variable.\n",
    "\n",
    "We can see that all input variables are numeric except the first, which is a string value.\n",
    "\n",
    "To keep data preparation simple, we will drop the first column from our models and focus on modeling the numeric input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-rogers",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "naughty-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
    "X, y = X.astype('float'), y.astype('float')\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "textile-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.askpython.com/python/built-in-methods/python-iloc-function#:~:text=Python%20iloc()%20function%20enables,a%20data%20frame%20or%20dataset.\n",
    "#https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html\n",
    "#X = dataframe.iloc[: , 1:8]\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prepared-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use iloc and select all rows (:) against the last column (-1):\n",
    "#y = dataframe.iloc[: ,-1:] \n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "driving-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4177\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "X, y = X.astype('float'), y.astype('float')\n",
    "n_features = X.shape[1]\n",
    "print(X.shape[0]) #Rows\n",
    "print(X.shape[1]) # Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "liable-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train test using train_test_split\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size =0.33 , random_state =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-coupon",
   "metadata": {},
   "source": [
    "We can then define an MLP neural network model.\n",
    "\n",
    "The model will have two hidden layers, **the first with 20 nodes** **and the second with 10 nodes**, both using ReLU activation and “he normal” weight initialization (a good practice). The number of layers and nodes were chosen arbitrarily.\n",
    "\n",
    "The output layer will have a single node for predicting a numeric value and a linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "consecutive-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Activation, Dense \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(20 , input_dim = n_features , activation = 'relu',kernel_initializer = 'he_normal'))\n",
    "model.add(Dense(10 , activation = 'relu',kernel_initializer = 'he_normal'))\n",
    "model.add(Dense(1 , activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cheap-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='mse', optimizer='adam') #mean-squared-error , Adam = momentum+SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sought-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2798/2798 - 0s - loss: 72.4171\n",
      "Epoch 2/150\n",
      "2798/2798 - 0s - loss: 22.7966\n",
      "Epoch 3/150\n",
      "2798/2798 - 0s - loss: 10.6451\n",
      "Epoch 4/150\n",
      "2798/2798 - 0s - loss: 9.6749\n",
      "Epoch 5/150\n",
      "2798/2798 - 0s - loss: 8.8557\n",
      "Epoch 6/150\n",
      "2798/2798 - 0s - loss: 8.2203\n",
      "Epoch 7/150\n",
      "2798/2798 - 0s - loss: 7.7734\n",
      "Epoch 8/150\n",
      "2798/2798 - 0s - loss: 7.4433\n",
      "Epoch 9/150\n",
      "2798/2798 - 0s - loss: 7.1905\n",
      "Epoch 10/150\n",
      "2798/2798 - 0s - loss: 6.9354\n",
      "Epoch 11/150\n",
      "2798/2798 - 0s - loss: 6.7577\n",
      "Epoch 12/150\n",
      "2798/2798 - 0s - loss: 6.5414\n",
      "Epoch 13/150\n",
      "2798/2798 - 0s - loss: 6.2976\n",
      "Epoch 14/150\n",
      "2798/2798 - 0s - loss: 6.0872\n",
      "Epoch 15/150\n",
      "2798/2798 - 0s - loss: 5.8842\n",
      "Epoch 16/150\n",
      "2798/2798 - 0s - loss: 5.7507\n",
      "Epoch 17/150\n",
      "2798/2798 - 0s - loss: 5.6194\n",
      "Epoch 18/150\n",
      "2798/2798 - 0s - loss: 5.4922\n",
      "Epoch 19/150\n",
      "2798/2798 - 0s - loss: 5.4805\n",
      "Epoch 20/150\n",
      "2798/2798 - 0s - loss: 5.3337\n",
      "Epoch 21/150\n",
      "2798/2798 - 0s - loss: 5.2353\n",
      "Epoch 22/150\n",
      "2798/2798 - 0s - loss: 5.1823\n",
      "Epoch 23/150\n",
      "2798/2798 - 0s - loss: 5.1416\n",
      "Epoch 24/150\n",
      "2798/2798 - 0s - loss: 5.0976\n",
      "Epoch 25/150\n",
      "2798/2798 - 0s - loss: 5.0737\n",
      "Epoch 26/150\n",
      "2798/2798 - 0s - loss: 5.0454\n",
      "Epoch 27/150\n",
      "2798/2798 - 0s - loss: 5.0177\n",
      "Epoch 28/150\n",
      "2798/2798 - 0s - loss: 4.9957\n",
      "Epoch 29/150\n",
      "2798/2798 - 0s - loss: 5.0111\n",
      "Epoch 30/150\n",
      "2798/2798 - 0s - loss: 5.0428\n",
      "Epoch 31/150\n",
      "2798/2798 - 0s - loss: 4.9806\n",
      "Epoch 32/150\n",
      "2798/2798 - 0s - loss: 4.9702\n",
      "Epoch 33/150\n",
      "2798/2798 - 0s - loss: 4.9660\n",
      "Epoch 34/150\n",
      "2798/2798 - 0s - loss: 4.9438\n",
      "Epoch 35/150\n",
      "2798/2798 - 0s - loss: 4.9473\n",
      "Epoch 36/150\n",
      "2798/2798 - 0s - loss: 4.9300\n",
      "Epoch 37/150\n",
      "2798/2798 - 0s - loss: 4.9164\n",
      "Epoch 38/150\n",
      "2798/2798 - 0s - loss: 4.9327\n",
      "Epoch 39/150\n",
      "2798/2798 - 0s - loss: 4.9355\n",
      "Epoch 40/150\n",
      "2798/2798 - 0s - loss: 4.9155\n",
      "Epoch 41/150\n",
      "2798/2798 - 0s - loss: 4.8993\n",
      "Epoch 42/150\n",
      "2798/2798 - 0s - loss: 4.9031\n",
      "Epoch 43/150\n",
      "2798/2798 - 0s - loss: 4.8839\n",
      "Epoch 44/150\n",
      "2798/2798 - 0s - loss: 4.9215\n",
      "Epoch 45/150\n",
      "2798/2798 - 0s - loss: 4.8978\n",
      "Epoch 46/150\n",
      "2798/2798 - 0s - loss: 4.8865\n",
      "Epoch 47/150\n",
      "2798/2798 - 0s - loss: 4.9287\n",
      "Epoch 48/150\n",
      "2798/2798 - 0s - loss: 4.8841\n",
      "Epoch 49/150\n",
      "2798/2798 - 0s - loss: 4.8610\n",
      "Epoch 50/150\n",
      "2798/2798 - 0s - loss: 4.8927\n",
      "Epoch 51/150\n",
      "2798/2798 - 0s - loss: 4.8952\n",
      "Epoch 52/150\n",
      "2798/2798 - 0s - loss: 4.8942\n",
      "Epoch 53/150\n",
      "2798/2798 - 0s - loss: 4.9062\n",
      "Epoch 54/150\n",
      "2798/2798 - 0s - loss: 4.8446\n",
      "Epoch 55/150\n",
      "2798/2798 - 0s - loss: 4.8694\n",
      "Epoch 56/150\n",
      "2798/2798 - 0s - loss: 4.8683\n",
      "Epoch 57/150\n",
      "2798/2798 - 0s - loss: 4.8535\n",
      "Epoch 58/150\n",
      "2798/2798 - 0s - loss: 4.8829\n",
      "Epoch 59/150\n",
      "2798/2798 - 0s - loss: 4.8564\n",
      "Epoch 60/150\n",
      "2798/2798 - 0s - loss: 4.8555\n",
      "Epoch 61/150\n",
      "2798/2798 - 0s - loss: 4.8217\n",
      "Epoch 62/150\n",
      "2798/2798 - 0s - loss: 4.8709\n",
      "Epoch 63/150\n",
      "2798/2798 - 0s - loss: 4.8341\n",
      "Epoch 64/150\n",
      "2798/2798 - 0s - loss: 4.8717\n",
      "Epoch 65/150\n",
      "2798/2798 - 0s - loss: 4.8479\n",
      "Epoch 66/150\n",
      "2798/2798 - 0s - loss: 4.8192\n",
      "Epoch 67/150\n",
      "2798/2798 - 0s - loss: 4.8120\n",
      "Epoch 68/150\n",
      "2798/2798 - 0s - loss: 4.8078\n",
      "Epoch 69/150\n",
      "2798/2798 - 0s - loss: 4.8244\n",
      "Epoch 70/150\n",
      "2798/2798 - 0s - loss: 4.8403\n",
      "Epoch 71/150\n",
      "2798/2798 - 0s - loss: 4.8141\n",
      "Epoch 72/150\n",
      "2798/2798 - 0s - loss: 4.7983\n",
      "Epoch 73/150\n",
      "2798/2798 - 0s - loss: 4.8153\n",
      "Epoch 74/150\n",
      "2798/2798 - 0s - loss: 4.8061\n",
      "Epoch 75/150\n",
      "2798/2798 - 0s - loss: 4.8604\n",
      "Epoch 76/150\n",
      "2798/2798 - 0s - loss: 4.7956\n",
      "Epoch 77/150\n",
      "2798/2798 - 0s - loss: 4.8202\n",
      "Epoch 78/150\n",
      "2798/2798 - 0s - loss: 4.8579\n",
      "Epoch 79/150\n",
      "2798/2798 - 0s - loss: 4.7954\n",
      "Epoch 80/150\n",
      "2798/2798 - 0s - loss: 4.8068\n",
      "Epoch 81/150\n",
      "2798/2798 - 0s - loss: 4.8338\n",
      "Epoch 82/150\n",
      "2798/2798 - 0s - loss: 4.7920\n",
      "Epoch 83/150\n",
      "2798/2798 - 0s - loss: 4.7873\n",
      "Epoch 84/150\n",
      "2798/2798 - 0s - loss: 4.8635\n",
      "Epoch 85/150\n",
      "2798/2798 - 0s - loss: 4.8101\n",
      "Epoch 86/150\n",
      "2798/2798 - 0s - loss: 4.7923\n",
      "Epoch 87/150\n",
      "2798/2798 - 0s - loss: 4.7854\n",
      "Epoch 88/150\n",
      "2798/2798 - 0s - loss: 4.8104\n",
      "Epoch 89/150\n",
      "2798/2798 - 0s - loss: 4.7806\n",
      "Epoch 90/150\n",
      "2798/2798 - 0s - loss: 4.8048\n",
      "Epoch 91/150\n",
      "2798/2798 - 0s - loss: 4.7786\n",
      "Epoch 92/150\n",
      "2798/2798 - 0s - loss: 4.8103\n",
      "Epoch 93/150\n",
      "2798/2798 - 0s - loss: 4.7939\n",
      "Epoch 94/150\n",
      "2798/2798 - 0s - loss: 4.7888\n",
      "Epoch 95/150\n",
      "2798/2798 - 0s - loss: 4.7492\n",
      "Epoch 96/150\n",
      "2798/2798 - 0s - loss: 4.7924\n",
      "Epoch 97/150\n",
      "2798/2798 - 0s - loss: 4.7969\n",
      "Epoch 98/150\n",
      "2798/2798 - 0s - loss: 4.7768\n",
      "Epoch 99/150\n",
      "2798/2798 - 0s - loss: 4.7630\n",
      "Epoch 100/150\n",
      "2798/2798 - 0s - loss: 4.7994\n",
      "Epoch 101/150\n",
      "2798/2798 - 0s - loss: 4.8099\n",
      "Epoch 102/150\n",
      "2798/2798 - 0s - loss: 4.7892\n",
      "Epoch 103/150\n",
      "2798/2798 - 0s - loss: 4.7752\n",
      "Epoch 104/150\n",
      "2798/2798 - 0s - loss: 4.7660\n",
      "Epoch 105/150\n",
      "2798/2798 - 0s - loss: 4.7724\n",
      "Epoch 106/150\n",
      "2798/2798 - 0s - loss: 4.7420\n",
      "Epoch 107/150\n",
      "2798/2798 - 0s - loss: 4.7777\n",
      "Epoch 108/150\n",
      "2798/2798 - 0s - loss: 4.7653\n",
      "Epoch 109/150\n",
      "2798/2798 - 0s - loss: 4.7766\n",
      "Epoch 110/150\n",
      "2798/2798 - 0s - loss: 4.7533\n",
      "Epoch 111/150\n",
      "2798/2798 - 0s - loss: 4.7462\n",
      "Epoch 112/150\n",
      "2798/2798 - 0s - loss: 4.7428\n",
      "Epoch 113/150\n",
      "2798/2798 - 0s - loss: 4.7522\n",
      "Epoch 114/150\n",
      "2798/2798 - 0s - loss: 4.8028\n",
      "Epoch 115/150\n",
      "2798/2798 - 0s - loss: 4.7380\n",
      "Epoch 116/150\n",
      "2798/2798 - 0s - loss: 4.7510\n",
      "Epoch 117/150\n",
      "2798/2798 - 0s - loss: 4.7376\n",
      "Epoch 118/150\n",
      "2798/2798 - 0s - loss: 4.7440\n",
      "Epoch 119/150\n",
      "2798/2798 - 0s - loss: 4.7470\n",
      "Epoch 120/150\n",
      "2798/2798 - 0s - loss: 4.7347\n",
      "Epoch 121/150\n",
      "2798/2798 - 0s - loss: 4.7462\n",
      "Epoch 122/150\n",
      "2798/2798 - 0s - loss: 4.7507\n",
      "Epoch 123/150\n",
      "2798/2798 - 0s - loss: 4.7815\n",
      "Epoch 124/150\n",
      "2798/2798 - 0s - loss: 4.7309\n",
      "Epoch 125/150\n",
      "2798/2798 - 0s - loss: 4.7244\n",
      "Epoch 126/150\n",
      "2798/2798 - 0s - loss: 4.7188\n",
      "Epoch 127/150\n",
      "2798/2798 - 0s - loss: 4.7176\n",
      "Epoch 128/150\n",
      "2798/2798 - 0s - loss: 4.7440\n",
      "Epoch 129/150\n",
      "2798/2798 - 0s - loss: 4.7335\n",
      "Epoch 130/150\n",
      "2798/2798 - 0s - loss: 4.7555\n",
      "Epoch 131/150\n",
      "2798/2798 - 0s - loss: 4.7442\n",
      "Epoch 132/150\n",
      "2798/2798 - 0s - loss: 4.7275\n",
      "Epoch 133/150\n",
      "2798/2798 - 0s - loss: 4.7158\n",
      "Epoch 134/150\n",
      "2798/2798 - 0s - loss: 4.7232\n",
      "Epoch 135/150\n",
      "2798/2798 - 0s - loss: 4.7394\n",
      "Epoch 136/150\n",
      "2798/2798 - 0s - loss: 4.7840\n",
      "Epoch 137/150\n",
      "2798/2798 - 0s - loss: 4.7254\n",
      "Epoch 138/150\n",
      "2798/2798 - 0s - loss: 4.7701\n",
      "Epoch 139/150\n",
      "2798/2798 - 0s - loss: 4.7180\n",
      "Epoch 140/150\n",
      "2798/2798 - 0s - loss: 4.7650\n",
      "Epoch 141/150\n",
      "2798/2798 - 0s - loss: 4.7151\n",
      "Epoch 142/150\n",
      "2798/2798 - 0s - loss: 4.7175\n",
      "Epoch 143/150\n",
      "2798/2798 - 0s - loss: 4.7093\n",
      "Epoch 144/150\n",
      "2798/2798 - 0s - loss: 4.7172\n",
      "Epoch 145/150\n",
      "2798/2798 - 0s - loss: 4.7078\n",
      "Epoch 146/150\n",
      "2798/2798 - 0s - loss: 4.7072\n",
      "Epoch 147/150\n",
      "2798/2798 - 0s - loss: 4.7285\n",
      "Epoch 148/150\n",
      "2798/2798 - 0s - loss: 4.7200\n",
      "Epoch 149/150\n",
      "2798/2798 - 0s - loss: 4.6907\n",
      "Epoch 150/150\n",
      "2798/2798 - 0s - loss: 4.6858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa315008ac8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "computational-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.551\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test set\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "yhat = model.predict(X_test)\n",
    "error = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-shuttle",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-puzzle",
   "metadata": {},
   "source": [
    "The abalone dataset can be framed as a classification problem where each “ring” integer is taken as a separate class label.\n",
    "\n",
    "The example and model are much the same as the above example for regression, with a few important changes.\n",
    "\n",
    "This requires first assigning a separate integer for each “ring” value, starting at 0 and ending at the total number of “classes” minus one.\n",
    "\n",
    "This can be achieved using the LabelEncoder.\n",
    "\n",
    "We can also record the total number of classes as the total number of unique encoded class values, which will be needed by the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "nutritional-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode strings to integer\n",
    "from sklearn import preprocessing\n",
    "y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "n_class = len(pd.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-replica",
   "metadata": {},
   "source": [
    "After splitting the data into train and test sets as before, we can define the model and change the number of outputs from the model to equal the number of classes and use the softmax activation function, common for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "concerned-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(n_class, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-brunei",
   "metadata": {},
   "source": [
    "Given we have encoded class labels as integer values, we can fit the model by minimizing the sparse categorical cross-entropy loss function, appropriate for multi-class classification tasks with integer encoded class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "certain-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-sandwich",
   "metadata": {},
   "source": [
    "After the model is fit on the training dataset as before, we can evaluate the performance of the model by calculating the classification accuracy on the hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "communist-fisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.057\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "yhat = model.predict(X_test)\n",
    "yhat = np.argmax(yhat, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-china",
   "metadata": {},
   "source": [
    "# Combined Regression and Classification Models\n",
    "\n",
    "In this section, we can develop a single MLP neural network model that can make both regression and classification predictions for a single input.\n",
    "\n",
    "This is called a multi-output model and can be developed using the functional Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "opposite-purpose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Epoch 1/150\n",
      "2798/2798 - 0s - loss: 86.6990 - dense_55_loss: 83.1833 - dense_56_loss: 3.2698\n",
      "Epoch 2/150\n",
      "2798/2798 - 0s - loss: 36.5393 - dense_55_loss: 33.4620 - dense_56_loss: 2.9229\n",
      "Epoch 3/150\n",
      "2798/2798 - 0s - loss: 13.3321 - dense_55_loss: 10.6753 - dense_56_loss: 2.6392\n",
      "Epoch 4/150\n",
      "2798/2798 - 0s - loss: 10.8706 - dense_55_loss: 8.3228 - dense_56_loss: 2.5527\n",
      "Epoch 5/150\n",
      "2798/2798 - 0s - loss: 10.5514 - dense_55_loss: 8.0116 - dense_56_loss: 2.5191\n",
      "Epoch 6/150\n",
      "2798/2798 - 0s - loss: 10.2564 - dense_55_loss: 7.7441 - dense_56_loss: 2.5023\n",
      "Epoch 7/150\n",
      "2798/2798 - 0s - loss: 9.9802 - dense_55_loss: 7.4908 - dense_56_loss: 2.4914\n",
      "Epoch 8/150\n",
      "2798/2798 - 0s - loss: 9.7149 - dense_55_loss: 7.2150 - dense_56_loss: 2.4769\n",
      "Epoch 9/150\n",
      "2798/2798 - 0s - loss: 9.4734 - dense_55_loss: 6.9737 - dense_56_loss: 2.4658\n",
      "Epoch 10/150\n",
      "2798/2798 - 0s - loss: 9.2510 - dense_55_loss: 6.8008 - dense_56_loss: 2.4577\n",
      "Epoch 11/150\n",
      "2798/2798 - 0s - loss: 9.0370 - dense_55_loss: 6.5614 - dense_56_loss: 2.4462\n",
      "Epoch 12/150\n",
      "2798/2798 - 0s - loss: 8.8565 - dense_55_loss: 6.3928 - dense_56_loss: 2.4351\n",
      "Epoch 13/150\n",
      "2798/2798 - 0s - loss: 8.7034 - dense_55_loss: 6.2511 - dense_56_loss: 2.4236\n",
      "Epoch 14/150\n",
      "2798/2798 - 0s - loss: 8.5732 - dense_55_loss: 6.1989 - dense_56_loss: 2.4212\n",
      "Epoch 15/150\n",
      "2798/2798 - 0s - loss: 8.4846 - dense_55_loss: 6.1255 - dense_56_loss: 2.4100\n",
      "Epoch 16/150\n",
      "2798/2798 - 0s - loss: 8.3542 - dense_55_loss: 5.9337 - dense_56_loss: 2.3985\n",
      "Epoch 17/150\n",
      "2798/2798 - 0s - loss: 8.2395 - dense_55_loss: 5.8214 - dense_56_loss: 2.3943\n",
      "Epoch 18/150\n",
      "2798/2798 - 0s - loss: 8.1331 - dense_55_loss: 5.7241 - dense_56_loss: 2.3878\n",
      "Epoch 19/150\n",
      "2798/2798 - 0s - loss: 8.0461 - dense_55_loss: 5.6517 - dense_56_loss: 2.3819\n",
      "Epoch 20/150\n",
      "2798/2798 - 0s - loss: 7.9358 - dense_55_loss: 5.5690 - dense_56_loss: 2.3811\n",
      "Epoch 21/150\n",
      "2798/2798 - 0s - loss: 7.8600 - dense_55_loss: 5.4721 - dense_56_loss: 2.3758\n",
      "Epoch 22/150\n",
      "2798/2798 - 0s - loss: 7.7758 - dense_55_loss: 5.3838 - dense_56_loss: 2.3704\n",
      "Epoch 23/150\n",
      "2798/2798 - 0s - loss: 7.6880 - dense_55_loss: 5.3319 - dense_56_loss: 2.3669\n",
      "Epoch 24/150\n",
      "2798/2798 - 0s - loss: 7.6055 - dense_55_loss: 5.2513 - dense_56_loss: 2.3647\n",
      "Epoch 25/150\n",
      "2798/2798 - 0s - loss: 7.5378 - dense_55_loss: 5.1786 - dense_56_loss: 2.3604\n",
      "Epoch 26/150\n",
      "2798/2798 - 0s - loss: 7.4686 - dense_55_loss: 5.1158 - dense_56_loss: 2.3584\n",
      "Epoch 27/150\n",
      "2798/2798 - 0s - loss: 7.4366 - dense_55_loss: 5.0722 - dense_56_loss: 2.3518\n",
      "Epoch 28/150\n",
      "2798/2798 - 0s - loss: 7.3613 - dense_55_loss: 4.9831 - dense_56_loss: 2.3506\n",
      "Epoch 29/150\n",
      "2798/2798 - 0s - loss: 7.3183 - dense_55_loss: 4.9549 - dense_56_loss: 2.3469\n",
      "Epoch 30/150\n",
      "2798/2798 - 0s - loss: 7.2773 - dense_55_loss: 4.9307 - dense_56_loss: 2.3426\n",
      "Epoch 31/150\n",
      "2798/2798 - 0s - loss: 7.2840 - dense_55_loss: 4.9628 - dense_56_loss: 2.3437\n",
      "Epoch 32/150\n",
      "2798/2798 - 0s - loss: 7.2151 - dense_55_loss: 4.8857 - dense_56_loss: 2.3399\n",
      "Epoch 33/150\n",
      "2798/2798 - 0s - loss: 7.1756 - dense_55_loss: 4.8263 - dense_56_loss: 2.3336\n",
      "Epoch 34/150\n",
      "2798/2798 - 0s - loss: 7.1546 - dense_55_loss: 4.8021 - dense_56_loss: 2.3312\n",
      "Epoch 35/150\n",
      "2798/2798 - 0s - loss: 7.1467 - dense_55_loss: 4.8132 - dense_56_loss: 2.3260\n",
      "Epoch 36/150\n",
      "2798/2798 - 0s - loss: 7.1337 - dense_55_loss: 4.8025 - dense_56_loss: 2.3238\n",
      "Epoch 37/150\n",
      "2798/2798 - 0s - loss: 7.1133 - dense_55_loss: 4.7966 - dense_56_loss: 2.3236\n",
      "Epoch 38/150\n",
      "2798/2798 - 0s - loss: 7.0980 - dense_55_loss: 4.8025 - dense_56_loss: 2.3211\n",
      "Epoch 39/150\n",
      "2798/2798 - 0s - loss: 7.0824 - dense_55_loss: 4.7480 - dense_56_loss: 2.3165\n",
      "Epoch 40/150\n",
      "2798/2798 - 0s - loss: 7.0780 - dense_55_loss: 4.7516 - dense_56_loss: 2.3114\n",
      "Epoch 41/150\n",
      "2798/2798 - 0s - loss: 7.0546 - dense_55_loss: 4.7314 - dense_56_loss: 2.3086\n",
      "Epoch 42/150\n",
      "2798/2798 - 0s - loss: 7.0437 - dense_55_loss: 4.7774 - dense_56_loss: 2.3109\n",
      "Epoch 43/150\n",
      "2798/2798 - 0s - loss: 7.0343 - dense_55_loss: 4.7313 - dense_56_loss: 2.3050\n",
      "Epoch 44/150\n",
      "2798/2798 - 0s - loss: 7.0390 - dense_55_loss: 4.7543 - dense_56_loss: 2.3024\n",
      "Epoch 45/150\n",
      "2798/2798 - 0s - loss: 7.0206 - dense_55_loss: 4.7133 - dense_56_loss: 2.2979\n",
      "Epoch 46/150\n",
      "2798/2798 - 0s - loss: 7.0237 - dense_55_loss: 4.7132 - dense_56_loss: 2.2939\n",
      "Epoch 47/150\n",
      "2798/2798 - 0s - loss: 7.0043 - dense_55_loss: 4.7008 - dense_56_loss: 2.2937\n",
      "Epoch 48/150\n",
      "2798/2798 - 0s - loss: 7.0112 - dense_55_loss: 4.7074 - dense_56_loss: 2.2869\n",
      "Epoch 49/150\n",
      "2798/2798 - 0s - loss: 7.0452 - dense_55_loss: 4.7522 - dense_56_loss: 2.2843\n",
      "Epoch 50/150\n",
      "2798/2798 - 0s - loss: 6.9853 - dense_55_loss: 4.7246 - dense_56_loss: 2.2855\n",
      "Epoch 51/150\n",
      "2798/2798 - 0s - loss: 6.9906 - dense_55_loss: 4.7049 - dense_56_loss: 2.2783\n",
      "Epoch 52/150\n",
      "2798/2798 - 0s - loss: 6.9768 - dense_55_loss: 4.7357 - dense_56_loss: 2.2773\n",
      "Epoch 53/150\n",
      "2798/2798 - 0s - loss: 6.9565 - dense_55_loss: 4.6774 - dense_56_loss: 2.2756\n",
      "Epoch 54/150\n",
      "2798/2798 - 0s - loss: 6.9309 - dense_55_loss: 4.6647 - dense_56_loss: 2.2634\n",
      "Epoch 55/150\n",
      "2798/2798 - 0s - loss: 6.9669 - dense_55_loss: 4.7129 - dense_56_loss: 2.2541\n",
      "Epoch 56/150\n",
      "2798/2798 - 0s - loss: 6.9117 - dense_55_loss: 4.6745 - dense_56_loss: 2.2435\n",
      "Epoch 57/150\n",
      "2798/2798 - 0s - loss: 6.9235 - dense_55_loss: 4.7024 - dense_56_loss: 2.2381\n",
      "Epoch 58/150\n",
      "2798/2798 - 0s - loss: 6.9418 - dense_55_loss: 4.7136 - dense_56_loss: 2.2247\n",
      "Epoch 59/150\n",
      "2798/2798 - 0s - loss: 6.8890 - dense_55_loss: 4.6572 - dense_56_loss: 2.2131\n",
      "Epoch 60/150\n",
      "2798/2798 - 0s - loss: 6.8578 - dense_55_loss: 4.6369 - dense_56_loss: 2.2019\n",
      "Epoch 61/150\n",
      "2798/2798 - 0s - loss: 6.8527 - dense_55_loss: 4.6443 - dense_56_loss: 2.1963\n",
      "Epoch 62/150\n",
      "2798/2798 - 0s - loss: 6.8251 - dense_55_loss: 4.6369 - dense_56_loss: 2.1847\n",
      "Epoch 63/150\n",
      "2798/2798 - 0s - loss: 6.8333 - dense_55_loss: 4.6578 - dense_56_loss: 2.1769\n",
      "Epoch 64/150\n",
      "2798/2798 - 0s - loss: 6.8309 - dense_55_loss: 4.6903 - dense_56_loss: 2.1703\n",
      "Epoch 65/150\n",
      "2798/2798 - 0s - loss: 6.8122 - dense_55_loss: 4.6400 - dense_56_loss: 2.1609\n",
      "Epoch 66/150\n",
      "2798/2798 - 0s - loss: 6.8250 - dense_55_loss: 4.6589 - dense_56_loss: 2.1529\n",
      "Epoch 67/150\n",
      "2798/2798 - 0s - loss: 6.7847 - dense_55_loss: 4.6252 - dense_56_loss: 2.1435\n",
      "Epoch 68/150\n",
      "2798/2798 - 0s - loss: 6.7553 - dense_55_loss: 4.6084 - dense_56_loss: 2.1367\n",
      "Epoch 69/150\n",
      "2798/2798 - 0s - loss: 6.7865 - dense_55_loss: 4.6374 - dense_56_loss: 2.1343\n",
      "Epoch 70/150\n",
      "2798/2798 - 0s - loss: 6.7616 - dense_55_loss: 4.6283 - dense_56_loss: 2.1272\n",
      "Epoch 71/150\n",
      "2798/2798 - 0s - loss: 6.7665 - dense_55_loss: 4.6292 - dense_56_loss: 2.1218\n",
      "Epoch 72/150\n",
      "2798/2798 - 0s - loss: 6.7327 - dense_55_loss: 4.6014 - dense_56_loss: 2.1165\n",
      "Epoch 73/150\n",
      "2798/2798 - 0s - loss: 6.7271 - dense_55_loss: 4.6001 - dense_56_loss: 2.1094\n",
      "Epoch 74/150\n",
      "2798/2798 - 0s - loss: 6.7190 - dense_55_loss: 4.6006 - dense_56_loss: 2.1053\n",
      "Epoch 75/150\n",
      "2798/2798 - 0s - loss: 6.7409 - dense_55_loss: 4.6336 - dense_56_loss: 2.1004\n",
      "Epoch 76/150\n",
      "2798/2798 - 0s - loss: 6.7705 - dense_55_loss: 4.6545 - dense_56_loss: 2.0939\n",
      "Epoch 77/150\n",
      "2798/2798 - 0s - loss: 6.7130 - dense_55_loss: 4.6415 - dense_56_loss: 2.0920\n",
      "Epoch 78/150\n",
      "2798/2798 - 0s - loss: 6.6889 - dense_55_loss: 4.5985 - dense_56_loss: 2.0868\n",
      "Epoch 79/150\n",
      "2798/2798 - 0s - loss: 6.7114 - dense_55_loss: 4.6251 - dense_56_loss: 2.0842\n",
      "Epoch 80/150\n",
      "2798/2798 - 0s - loss: 6.6910 - dense_55_loss: 4.5903 - dense_56_loss: 2.0796\n",
      "Epoch 81/150\n",
      "2798/2798 - 0s - loss: 6.6702 - dense_55_loss: 4.5806 - dense_56_loss: 2.0740\n",
      "Epoch 82/150\n",
      "2798/2798 - 0s - loss: 6.6903 - dense_55_loss: 4.6526 - dense_56_loss: 2.0747\n",
      "Epoch 83/150\n",
      "2798/2798 - 0s - loss: 6.6740 - dense_55_loss: 4.6099 - dense_56_loss: 2.0697\n",
      "Epoch 84/150\n",
      "2798/2798 - 0s - loss: 6.6576 - dense_55_loss: 4.5699 - dense_56_loss: 2.0652\n",
      "Epoch 85/150\n",
      "2798/2798 - 0s - loss: 6.6509 - dense_55_loss: 4.5803 - dense_56_loss: 2.0604\n",
      "Epoch 86/150\n",
      "2798/2798 - 0s - loss: 6.6399 - dense_55_loss: 4.5667 - dense_56_loss: 2.0575\n",
      "Epoch 87/150\n",
      "2798/2798 - 0s - loss: 6.6386 - dense_55_loss: 4.5796 - dense_56_loss: 2.0584\n",
      "Epoch 88/150\n",
      "2798/2798 - 0s - loss: 6.6286 - dense_55_loss: 4.5646 - dense_56_loss: 2.0555\n",
      "Epoch 89/150\n",
      "2798/2798 - 0s - loss: 6.6200 - dense_55_loss: 4.5642 - dense_56_loss: 2.0501\n",
      "Epoch 90/150\n",
      "2798/2798 - 0s - loss: 6.6108 - dense_55_loss: 4.5568 - dense_56_loss: 2.0485\n",
      "Epoch 91/150\n",
      "2798/2798 - 0s - loss: 6.6095 - dense_55_loss: 4.5533 - dense_56_loss: 2.0443\n",
      "Epoch 92/150\n",
      "2798/2798 - 0s - loss: 6.6282 - dense_55_loss: 4.5789 - dense_56_loss: 2.0408\n",
      "Epoch 93/150\n",
      "2798/2798 - 0s - loss: 6.5923 - dense_55_loss: 4.5366 - dense_56_loss: 2.0386\n",
      "Epoch 94/150\n",
      "2798/2798 - 0s - loss: 6.6305 - dense_55_loss: 4.5867 - dense_56_loss: 2.0381\n",
      "Epoch 95/150\n",
      "2798/2798 - 0s - loss: 6.5921 - dense_55_loss: 4.5638 - dense_56_loss: 2.0375\n",
      "Epoch 96/150\n",
      "2798/2798 - 0s - loss: 6.6033 - dense_55_loss: 4.5756 - dense_56_loss: 2.0342\n",
      "Epoch 97/150\n",
      "2798/2798 - 0s - loss: 6.5798 - dense_55_loss: 4.6252 - dense_56_loss: 2.0334\n",
      "Epoch 98/150\n",
      "2798/2798 - 0s - loss: 6.5869 - dense_55_loss: 4.5401 - dense_56_loss: 2.0285\n",
      "Epoch 99/150\n",
      "2798/2798 - 0s - loss: 6.5812 - dense_55_loss: 4.5450 - dense_56_loss: 2.0285\n",
      "Epoch 100/150\n",
      "2798/2798 - 0s - loss: 6.5614 - dense_55_loss: 4.5260 - dense_56_loss: 2.0225\n",
      "Epoch 101/150\n",
      "2798/2798 - 0s - loss: 6.5893 - dense_55_loss: 4.5607 - dense_56_loss: 2.0221\n",
      "Epoch 102/150\n",
      "2798/2798 - 0s - loss: 6.5605 - dense_55_loss: 4.5274 - dense_56_loss: 2.0221\n",
      "Epoch 103/150\n",
      "2798/2798 - 0s - loss: 6.5620 - dense_55_loss: 4.5415 - dense_56_loss: 2.0200\n",
      "Epoch 104/150\n",
      "2798/2798 - 0s - loss: 6.5891 - dense_55_loss: 4.5627 - dense_56_loss: 2.0217\n",
      "Epoch 105/150\n",
      "2798/2798 - 0s - loss: 6.5777 - dense_55_loss: 4.5505 - dense_56_loss: 2.0173\n",
      "Epoch 106/150\n",
      "2798/2798 - 0s - loss: 6.5627 - dense_55_loss: 4.5289 - dense_56_loss: 2.0157\n",
      "Epoch 107/150\n",
      "2798/2798 - 0s - loss: 6.5847 - dense_55_loss: 4.5842 - dense_56_loss: 2.0181\n",
      "Epoch 108/150\n",
      "2798/2798 - 0s - loss: 6.5365 - dense_55_loss: 4.5597 - dense_56_loss: 2.0121\n",
      "Epoch 109/150\n",
      "2798/2798 - 0s - loss: 6.5584 - dense_55_loss: 4.5421 - dense_56_loss: 2.0107\n",
      "Epoch 110/150\n",
      "2798/2798 - 0s - loss: 6.5345 - dense_55_loss: 4.5315 - dense_56_loss: 2.0111\n",
      "Epoch 111/150\n",
      "2798/2798 - 0s - loss: 6.5896 - dense_55_loss: 4.5730 - dense_56_loss: 2.0102\n",
      "Epoch 112/150\n",
      "2798/2798 - 0s - loss: 6.5467 - dense_55_loss: 4.5714 - dense_56_loss: 2.0107\n",
      "Epoch 113/150\n",
      "2798/2798 - 0s - loss: 6.5312 - dense_55_loss: 4.5439 - dense_56_loss: 2.0078\n",
      "Epoch 114/150\n",
      "2798/2798 - 0s - loss: 6.5951 - dense_55_loss: 4.5814 - dense_56_loss: 2.0058\n",
      "Epoch 115/150\n",
      "2798/2798 - 0s - loss: 6.5393 - dense_55_loss: 4.5179 - dense_56_loss: 2.0063\n",
      "Epoch 116/150\n",
      "2798/2798 - 0s - loss: 6.5330 - dense_55_loss: 4.5683 - dense_56_loss: 2.0053\n",
      "Epoch 117/150\n",
      "2798/2798 - 0s - loss: 6.5294 - dense_55_loss: 4.5029 - dense_56_loss: 2.0004\n",
      "Epoch 118/150\n",
      "2798/2798 - 0s - loss: 6.5893 - dense_55_loss: 4.5722 - dense_56_loss: 2.0004\n",
      "Epoch 119/150\n",
      "2798/2798 - 0s - loss: 6.5661 - dense_55_loss: 4.5747 - dense_56_loss: 2.0004\n",
      "Epoch 120/150\n",
      "2798/2798 - 0s - loss: 6.5059 - dense_55_loss: 4.5246 - dense_56_loss: 2.0024\n",
      "Epoch 121/150\n",
      "2798/2798 - 0s - loss: 6.5165 - dense_55_loss: 4.5180 - dense_56_loss: 1.9970\n",
      "Epoch 122/150\n",
      "2798/2798 - 0s - loss: 6.4922 - dense_55_loss: 4.4915 - dense_56_loss: 1.9976\n",
      "Epoch 123/150\n",
      "2798/2798 - 0s - loss: 6.5288 - dense_55_loss: 4.5223 - dense_56_loss: 1.9946\n",
      "Epoch 124/150\n",
      "2798/2798 - 0s - loss: 6.5194 - dense_55_loss: 4.5250 - dense_56_loss: 1.9966\n",
      "Epoch 125/150\n",
      "2798/2798 - 0s - loss: 6.5274 - dense_55_loss: 4.5144 - dense_56_loss: 1.9918\n",
      "Epoch 126/150\n",
      "2798/2798 - 0s - loss: 6.5119 - dense_55_loss: 4.5369 - dense_56_loss: 1.9946\n",
      "Epoch 127/150\n",
      "2798/2798 - 0s - loss: 6.5052 - dense_55_loss: 4.5353 - dense_56_loss: 1.9959\n",
      "Epoch 128/150\n",
      "2798/2798 - 0s - loss: 6.5167 - dense_55_loss: 4.5245 - dense_56_loss: 1.9932\n",
      "Epoch 129/150\n",
      "2798/2798 - 0s - loss: 6.4928 - dense_55_loss: 4.4906 - dense_56_loss: 1.9913\n",
      "Epoch 130/150\n",
      "2798/2798 - 0s - loss: 6.5108 - dense_55_loss: 4.5510 - dense_56_loss: 1.9943\n",
      "Epoch 131/150\n",
      "2798/2798 - 0s - loss: 6.4973 - dense_55_loss: 4.4902 - dense_56_loss: 1.9878\n",
      "Epoch 132/150\n",
      "2798/2798 - 0s - loss: 6.4958 - dense_55_loss: 4.4940 - dense_56_loss: 1.9901\n",
      "Epoch 133/150\n",
      "2798/2798 - 0s - loss: 6.4954 - dense_55_loss: 4.5362 - dense_56_loss: 1.9893\n",
      "Epoch 134/150\n",
      "2798/2798 - 0s - loss: 6.4639 - dense_55_loss: 4.4855 - dense_56_loss: 1.9884\n",
      "Epoch 135/150\n",
      "2798/2798 - 0s - loss: 6.4897 - dense_55_loss: 4.5215 - dense_56_loss: 1.9898\n",
      "Epoch 136/150\n",
      "2798/2798 - 0s - loss: 6.4939 - dense_55_loss: 4.5044 - dense_56_loss: 1.9853\n",
      "Epoch 137/150\n",
      "2798/2798 - 0s - loss: 6.4999 - dense_55_loss: 4.5013 - dense_56_loss: 1.9846\n",
      "Epoch 138/150\n",
      "2798/2798 - 0s - loss: 6.4812 - dense_55_loss: 4.5031 - dense_56_loss: 1.9855\n",
      "Epoch 139/150\n",
      "2798/2798 - 0s - loss: 6.4907 - dense_55_loss: 4.5330 - dense_56_loss: 1.9824\n",
      "Epoch 140/150\n",
      "2798/2798 - 0s - loss: 6.5502 - dense_55_loss: 4.5855 - dense_56_loss: 1.9870\n",
      "Epoch 141/150\n",
      "2798/2798 - 0s - loss: 6.4854 - dense_55_loss: 4.4861 - dense_56_loss: 1.9814\n",
      "Epoch 142/150\n",
      "2798/2798 - 0s - loss: 6.4941 - dense_55_loss: 4.4970 - dense_56_loss: 1.9777\n",
      "Epoch 143/150\n",
      "2798/2798 - 0s - loss: 6.4693 - dense_55_loss: 4.4660 - dense_56_loss: 1.9804\n",
      "Epoch 144/150\n",
      "2798/2798 - 0s - loss: 6.4691 - dense_55_loss: 4.4935 - dense_56_loss: 1.9798\n",
      "Epoch 145/150\n",
      "2798/2798 - 0s - loss: 6.4798 - dense_55_loss: 4.4912 - dense_56_loss: 1.9808\n",
      "Epoch 146/150\n",
      "2798/2798 - 0s - loss: 6.4658 - dense_55_loss: 4.4753 - dense_56_loss: 1.9764\n",
      "Epoch 147/150\n",
      "2798/2798 - 0s - loss: 6.4745 - dense_55_loss: 4.4955 - dense_56_loss: 1.9796\n",
      "Epoch 148/150\n",
      "2798/2798 - 0s - loss: 6.4650 - dense_55_loss: 4.4792 - dense_56_loss: 1.9757\n",
      "Epoch 149/150\n",
      "2798/2798 - 0s - loss: 6.4781 - dense_55_loss: 4.5004 - dense_56_loss: 1.9772\n",
      "Epoch 150/150\n",
      "2798/2798 - 0s - loss: 6.5106 - dense_55_loss: 4.5521 - dense_56_loss: 1.9811\n",
      "MAE: 1.515\n",
      "Accuracy: 0.259\n"
     ]
    }
   ],
   "source": [
    "# mlp for combined regression and classification predictions on the abalone dataset\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
    "X, y = X.astype('float'), y.astype('float')\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# encode strings to integer\n",
    "y_class = LabelEncoder().fit_transform(y)\n",
    "n_class = len(unique(y_class))\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_class, test_size=0.33, random_state=1)\n",
    "# input\n",
    "visible = Input(shape=(n_features,))\n",
    "hidden1 = Dense(20, activation='relu', kernel_initializer='he_normal')(visible)\n",
    "hidden2 = Dense(10, activation='relu', kernel_initializer='he_normal')(hidden1)\n",
    "# regression output\n",
    "out_reg = Dense(1, activation='linear')(hidden2)\n",
    "# classification output\n",
    "out_clas = Dense(n_class, activation='softmax')(hidden2)\n",
    "# define model\n",
    "model = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
    "# compile the keras model\n",
    "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\n",
    "# plot graph of model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, [y_train,y_train_class], epochs=150, batch_size=32, verbose=2)\n",
    "# make predictions on test set\n",
    "yhat1, yhat2 = model.predict(X_test)\n",
    "# calculate error for regression model\n",
    "error = mean_absolute_error(y_test, yhat1)\n",
    "print('MAE: %.3f' % error)\n",
    "# evaluate accuracy for classification model\n",
    "yhat2 = argmax(yhat2, axis=-1).astype('int')\n",
    "acc = accuracy_score(y_test_class, yhat2)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-interval",
   "metadata": {},
   "source": [
    "---\n",
    "https://machinelearningmastery.com/deep-learning-models-for-multi-output-regression/\n",
    "\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
